{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdnvN5D9Y0gQX2l2v2ik7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "253d0e8e20b548cb8eb2c6a9d891ca91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_598d0a36232d441bad578f7439d0c271",
              "IPY_MODEL_65713a2f3510435287e1151cdf453e5c",
              "IPY_MODEL_5976dbf55b754caabab10543b668c4a0"
            ],
            "layout": "IPY_MODEL_09179c35871c42379e0a6e66923acc4d"
          }
        },
        "598d0a36232d441bad578f7439d0c271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5995c6d16d7348269a68f0924ea86e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_a52aaba2fb144f899e04cfa01ba4dabd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "65713a2f3510435287e1151cdf453e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f876cb9813241f49b3cf8fbf0c07653",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d97ab655075e41089de6d34b6201952a",
            "value": 2
          }
        },
        "5976dbf55b754caabab10543b668c4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_957cc9cc24864a94817f6ae3dcda1c63",
            "placeholder": "​",
            "style": "IPY_MODEL_5118160481494bda9de95ab663702492",
            "value": " 2/2 [00:27&lt;00:00, 12.25s/it]"
          }
        },
        "09179c35871c42379e0a6e66923acc4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5995c6d16d7348269a68f0924ea86e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a52aaba2fb144f899e04cfa01ba4dabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f876cb9813241f49b3cf8fbf0c07653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97ab655075e41089de6d34b6201952a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "957cc9cc24864a94817f6ae3dcda1c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5118160481494bda9de95ab663702492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/buyers-ID-manual-project/blob/main/process_studies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook extracts and cleans text from PDF reports for use in training the buyer profile extractor model.**"
      ],
      "metadata": {
        "id": "h3140H82TCGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add input and output examples to jsonl file**"
      ],
      "metadata": {
        "id": "aSaUOBcBZMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/alex-jk/buyers-ID-manual-project.git\n",
        "%cd buyers-ID-manual-project\n",
        "!ls"
      ],
      "metadata": {
        "id": "Gi3MKqB7cGcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae42e36b-6347-43ff-cd3b-bdcdaabf1b9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'buyers-ID-manual-project'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 55 (delta 20), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (55/55), 2.83 MiB | 13.40 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "/content/buyers-ID-manual-project\n",
            "data\t\t\t      LICENSE\t\t     README.md\n",
            "extraction_utils.py\t      process_studies.ipynb\n",
            "extract_text_from_pdfs.ipynb  prompt_template.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# Buyers were willing to travel significant distances to reach underage victims.\n",
        "# One man cited the Atlanta airport as a regular meeting place.\n",
        "# \"\"\"\n",
        "\n",
        "# output_text = \"\"\"\n",
        "# Buyers are often mobile and willing to travel, including to major hubs like the Atlanta airport.\n",
        "# \"\"\"\n",
        "\n",
        "# entry = {\"input\": input_text.strip(), \"output\": output_text.strip()}\n",
        "\n",
        "# with open(\"data/labeled_chunks.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "#     f.write(json.dumps(entry) + \"\\n\")"
      ],
      "metadata": {
        "id": "-l9bAG6lTEYW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "FHHdr7ZZTYe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific compatible versions for torch 2.3.1 / cu121\n",
        "!pip install torch==2.3.1+cu121 torchaudio==2.3.1+cu121 torchvision==0.18.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Install the other pinned versions\n",
        "!pip install transformers==4.41.2 accelerate==0.31.0 --no-deps\n",
        "\n",
        "# Install compatible tokenizers version LAST to ensure it sticks\n",
        "!pip install tokenizers==0.19.1\n",
        "\n",
        "# Ensure base dependencies are present (redundant ok)\n",
        "!pip install bitsandbytes sentencepiece\n",
        "\n",
        "print(\"\\nInstalled specific package versions (including compatible tokenizers). PLEASE RESTART RUNTIME NOW.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTTOXwDwcuaQ",
        "outputId": "1e6d4073-7c1a-415a-898d-58074b91261f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.3.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.3.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.18.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1+cu121)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1+cu121)\n",
            "  Downloading triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cu121) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cu121) (11.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1+cu121) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.1+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.1+cu121) (1.3.0)\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1+cu121 torchaudio-2.3.1+cu121 torchvision-0.18.1+cu121 triton-2.3.1\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.31.0\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, accelerate\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.5.2\n",
            "    Uninstalling accelerate-1.5.2:\n",
            "      Successfully uninstalled accelerate-1.5.2\n",
            "Successfully installed accelerate-0.31.0 transformers-4.41.2\n",
            "Collecting tokenizers==0.19.1\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.19.1) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.1.31)\n",
            "Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "Successfully installed tokenizers-0.19.1\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n",
            "\n",
            "Installed specific package versions (including compatible tokenizers). PLEASE RESTART RUNTIME NOW.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "id": "yCeel7fiPtID"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read input output file and write to jsonl**"
      ],
      "metadata": {
        "id": "YPvesEjPT4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_df = pd.read_csv(\"data/buyers_id_manual_input_output.csv\")\n",
        "\n",
        "print(input_output_df.shape)\n",
        "print(input_output_df.columns)\n",
        "print(\"\\n-------------------------------\\n\")\n",
        "print(input_output_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u-YHG4rTcu7",
        "outputId": "4f29e8a1-769e-42c5-9d78-3f8a3383ae9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 3)\n",
            "Index(['Study', 'Input', 'Output'], dtype='object')\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "                                    Study  \\\n",
            "0  The Shapiro Group Georgia Demand Study   \n",
            "1  The Shapiro Group Georgia Demand Study   \n",
            "2  The Shapiro Group Georgia Demand Study   \n",
            "3  The Shapiro Group Georgia Demand Study   \n",
            "4  The Shapiro Group Georgia Demand Study   \n",
            "\n",
            "                                               Input  \\\n",
            "0  Almost half these men are the age 30-39, with ...   \n",
            "1  The data clearly debunk the myth that CSEC is ...   \n",
            "2  Not only are 65% of men who buy sex with young...   \n",
            "3  Craigslist is by far the most efficient medium...   \n",
            "4  While many of the men who exploit these childr...   \n",
            "\n",
            "                                              Output  \n",
            "0  Almost half these men are the age 30-39, with ...  \n",
            "1  Men who\\nrespond to advertisements for sex wit...  \n",
            "2  65% of men who buy sex with young females do s...  \n",
            "3  Buyers respond to Craigslist ads 3 times more ...  \n",
            "4  Nearly half of buyers are willing to pay for s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_df = input_output_df.drop_duplicates(subset=[\"Study\", \"Input\", \"Output\"])\n",
        "\n",
        "# Convert to JSONL\n",
        "with open(\"data/labeled_chunks.jsonl\", \"w\") as f:\n",
        "    for _, row in input_output_df.iterrows():\n",
        "        json_obj = {\n",
        "            \"study\": row[\"Study\"],\n",
        "            \"input\": row[\"Input\"],\n",
        "            \"output\": row[\"Output\"]\n",
        "        }\n",
        "        f.write(json.dumps(json_obj) + \"\\n\")"
      ],
      "metadata": {
        "id": "jC56MIoNVPMZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models for text extraction**"
      ],
      "metadata": {
        "id": "z7s-W-gNZPi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import extraction_utils\n",
        "\n",
        "# Reload the entire module\n",
        "importlib.reload(extraction_utils)\n",
        "print(\"Module 'extraction_utils' reloaded.\")\n",
        "\n",
        "from extraction_utils import create_prompt_messages, extract_information\n",
        "\n",
        "print(\"Functions 'create_prompt_messages' and 'extract_information' imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRHpTCINVOxX",
        "outputId": "f764015f-8c44-4a79-a14b-742657427bb3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module 'extraction_utils' reloaded.\n",
            "Functions 'create_prompt_messages' and 'extract_information' imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgJ0s70ynDtG",
        "outputId": "4e3caf99-2061-4f11-9014-ea122abc1a3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "YYtd4ZYUZRw2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# --- Load Model and Tokenizer ---\n",
        "# Load the model with 4-bit quantization to save memory (requires bitsandbytes)\n",
        "# Use device_map=\"auto\" to automatically use GPU if available\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",         # Use GPU if available, otherwise CPU\n",
        "        torch_dtype=\"auto\",        # Automatically select appropriate dtype\n",
        "        trust_remote_code=True,    # Phi-3 requires this\n",
        "        # Optional: uncomment below for 4-bit loading (needs bitsandbytes)\n",
        "        # load_in_4bit=True,\n",
        "        # bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    print(f\"Model '{model_id}' loaded successfully.\")\n",
        "\n",
        "    # --- Create a Hugging Face Pipeline for easier text generation ---\n",
        "    # Note: max_new_tokens controls how long the generated output can be. Adjust as needed.\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        # Adjust max_new_tokens if your extracted text might be longer\n",
        "        # Needs to be long enough for the longest expected extraction + \"NONE\"\n",
        "        max_new_tokens=256,\n",
        "        # Temperature=0 means more deterministic output, higher means more creative/random\n",
        "        # temperature=0.0,\n",
        "        # top_p=0.95, # Optional: nucleus sampling\n",
        "        do_sample=False # Set to False for more deterministic extraction\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or creating pipeline: {e}\")\n",
        "    print(\"Ensure you have sufficient RAM/VRAM and necessary libraries installed.\")\n",
        "    print(\"Consider using Google Colab with a T4 GPU runtime.\")\n",
        "    # Exit if model loading fails\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "253d0e8e20b548cb8eb2c6a9d891ca91",
            "598d0a36232d441bad578f7439d0c271",
            "65713a2f3510435287e1151cdf453e5c",
            "5976dbf55b754caabab10543b668c4a0",
            "09179c35871c42379e0a6e66923acc4d",
            "5995c6d16d7348269a68f0924ea86e9f",
            "a52aaba2fb144f899e04cfa01ba4dabd",
            "2f876cb9813241f49b3cf8fbf0c07653",
            "d97ab655075e41089de6d34b6201952a",
            "957cc9cc24864a94817f6ae3dcda1c63",
            "5118160481494bda9de95ab663702492"
          ]
        },
        "id": "86Z4-8tTnLC_",
        "outputId": "cc17a4a4-0156-413b-ca07-b82e0c89637a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "253d0e8e20b548cb8eb2c6a9d891ca91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'microsoft/Phi-3-mini-4k-instruct' loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usage**"
      ],
      "metadata": {
        "id": "XywDoaLuZcBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_1 = \"Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\"\n",
        "\n",
        "input_text_2 = \"The data clearly debunk the myth that CSEC is a problem relegated to the urban core. Men who respond to advertisements for sex with young females come from all over metro Atlanta, the geographic market where the advertisements in this study were targeted.\"\n",
        "\n",
        "input_text_3 = \"This paragraph discusses unrelated economic factors in the region and contains no information about buyers or traffickers.\"\n",
        "\n",
        "input_text_4 = \"Research indicates traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\""
      ],
      "metadata": {
        "id": "uVOQqRnEZeOg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_buyers_profiles = \"prompt_template.txt\""
      ],
      "metadata": {
        "id": "6oc9NC9diERy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/buyers-ID-manual-project\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bRDK6S1jYPu",
        "outputId": "cf76f9d2-1b49-4914-b1ba-f899469cfc29"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/buyers-ID-manual-project\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 2.23 KiB | 1.11 MiB/s, done.\n",
            "From https://github.com/alex-jk/buyers-ID-manual-project\n",
            "   1d96c36..1ecdafd  main       -> origin/main\n",
            "Updating 1d96c36..1ecdafd\n",
            "Fast-forward\n",
            " extraction_utils.py | 96 \u001b[32m+++++++++++++++++++++++++++++++\u001b[m\u001b[31m----------------------\u001b[m\n",
            " 1 file changed, 57 insertions(+), 39 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/buyers-ID-manual-project/extraction_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bljelaGjjEqQ",
        "outputId": "c07a2518-f2be-43dd-8b6c-6c8630f04aed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import os\n",
            "\n",
            "# --- Function to Load a Specific Prompt File ---\n",
            "def load_system_prompt(prompt_filename):\n",
            "    \"\"\"Loads system prompt text from a specified file.\"\"\"\n",
            "    system_prompt = \"\"\n",
            "    try:\n",
            "        # Assumes prompt file is in the same directory as this script\n",
            "        script_dir = os.path.dirname(__file__)\n",
            "        prompt_path = os.path.join(script_dir, prompt_filename)\n",
            "        with open(prompt_path, 'r', encoding='utf-8') as f:\n",
            "            system_prompt = f.read()\n",
            "        if not system_prompt:\n",
            "            print(f\"WARNING: Prompt file loaded but is empty: {prompt_path}\")\n",
            "            return f\"ERROR: Prompt file empty {prompt_filename}\" # Return specific error\n",
            "    except FileNotFoundError:\n",
            "        print(f\"ERROR: Prompt file not found at {prompt_path}\")\n",
            "        return f\"ERROR: Prompt file not found {prompt_filename}\" # Return specific error\n",
            "    except Exception as e:\n",
            "        print(f\"ERROR: Could not read prompt file {prompt_path}: {e}\")\n",
            "        return f\"ERROR: Could not read prompt file {prompt_filename}\" # Return specific error\n",
            "    return system_prompt\n",
            "\n",
            "# --- Function to Create Prompt Messages (Now uses loaded prompt) ---\n",
            "def create_prompt_messages(input_chunk, system_prompt_text):\n",
            "    \"\"\"Creates the chat message structure using the provided system prompt text.\"\"\"\n",
            "\n",
            "    # Check if system_prompt_text indicates an error from loading\n",
            "    if \"ERROR:\" in system_prompt_text:\n",
            "         print(f\"WARNING: Using error message as system prompt: {system_prompt_text}\")\n",
            "         # Optionally, you could return an error flag or raise an exception here too\n",
            "\n",
            "    # User prompt remains dynamic\n",
            "    user_prompt = f\"\"\"Now, perform the task for the following Input Text:\n",
            "\n",
            "Input Text:\n",
            "\"{input_chunk}\"\n",
            "\n",
            "Extracted Information:\n",
            "\"\"\"\n",
            "    messages = [\n",
            "        {\"role\": \"system\", \"content\": system_prompt_text}, # Use the passed-in system prompt\n",
            "        {\"role\": \"user\", \"content\": user_prompt},\n",
            "    ]\n",
            "    return messages\n",
            "\n",
            "# --- Function to Extract Information (Now accepts prompt filename) ---\n",
            "def extract_information(text_chunk, generation_pipeline, prompt_filename):\n",
            "    \"\"\"\n",
            "    Uses the pipeline to extract info based on the prompt specified by prompt_filename.\n",
            "    \"\"\"\n",
            "    if generation_pipeline is None or not callable(generation_pipeline):\n",
            "        print(\"Pipeline not initialized or not valid.\")\n",
            "        return \"Error: Pipeline not available or invalid.\"\n",
            "\n",
            "    # 1. Load the specific system prompt for this task\n",
            "    system_prompt_text = load_system_prompt(prompt_filename)\n",
            "    if \"ERROR:\" in system_prompt_text:\n",
            "         return f\"Error: Failed to load system prompt from '{prompt_filename}'.\" # Return early if prompt fails\n",
            "\n",
            "    # 2. Create messages using the loaded system prompt\n",
            "    messages = create_prompt_messages(text_chunk, system_prompt_text)\n",
            "\n",
            "    # 3. Call the pipeline and process output (same logic as before)\n",
            "    try:\n",
            "        outputs = generation_pipeline(messages, return_full_text=False)\n",
            "\n",
            "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
            "             generated_text = outputs[0]['generated_text'].strip()\n",
            "        else:\n",
            "             print(f\"Warning: Unexpected output format from pipeline: {outputs}\")\n",
            "             generated_text = \"Error: Unexpected pipeline output format.\"\n",
            "\n",
            "        if isinstance(generated_text, str): # Ensure it's a string before string methods\n",
            "            if generated_text.startswith('\"') and generated_text.endswith('\"'):\n",
            "                 generated_text = generated_text[1:-1]\n",
            "            if generated_text.startswith(\"'\") and generated_text.endswith(\"'\"):\n",
            "                 generated_text = generated_text[1:-1]\n",
            "            instruction_tail = \"Extracted Information:\"\n",
            "            if generated_text.startswith(instruction_tail):\n",
            "                generated_text = generated_text[len(instruction_tail):].strip()\n",
            "            if not generated_text or generated_text.lower() == \"none\":\n",
            "                return \"NONE\"\n",
            "\n",
            "        return generated_text\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"Error during generation pipeline call for {prompt_filename}: {e}\")\n",
            "        return f\"Error: Exception during generation. ({type(e).__name__})\"\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"Error during generation: {e}\")\n",
            "        return f\"Error: Could not generate output. ({e})\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Extracting Information ---\")\n",
        "\n",
        "# Check if the pipeline object exists before trying to use it\n",
        "if 'pipe' in locals() and pipe is not None:\n",
        "    print(f\"\\nRunning prompt: {prompt_buyers_profiles}\")\n",
        "\n",
        "    print(f\"\\nInput 1:\\n{input_text_1}\")\n",
        "    extraction_1 = extract_information(input_text_1, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 1:\\n{extraction_1}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 2:\\n{input_text_2}\")\n",
        "    extraction_2 = extract_information(input_text_2, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 2:\\n{extraction_2}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 3:\\n{input_text_3}\")\n",
        "    extraction_3 = extract_information(input_text_3, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 3:\\n{extraction_3}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 4:\\n{input_text_4}\")\n",
        "    extraction_4 = extract_information(input_text_4, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 4:\\n{extraction_4}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: The 'pipe' object (text generation pipeline) was not found or not created successfully.\")\n",
        "    print(\"Please ensure the model loading cell was run successfully after importing functions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju44xbqscRuN",
        "outputId": "6d815bd4-4030-49d3-b224-0dd114962f29"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extracting Information ---\n",
            "\n",
            "Running prompt: prompt_template.txt\n",
            "\n",
            "Input 1:\n",
            "Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\n",
            "\n",
            "Extraction 1:\n",
            "Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\n",
            "--------------------\n",
            "\n",
            "Input 2:\n",
            "The data clearly debunk the myth that CSEC is a problem relegated to the urban core. Men who respond to advertisements for sex with young females come from all over metro Atlanta, the geographic market where the advertisements in this study were targeted.\n",
            "\n",
            "Extraction 2:\n",
            "Men who respond to advertisements for sex with young females come from all over metro Atlanta.\n",
            "--------------------\n",
            "\n",
            "Input 3:\n",
            "This paragraph discusses unrelated economic factors in the region and contains no information about buyers or traffickers.\n",
            "\n",
            "Extraction 3:\n",
            "NONE\n",
            "--------------------\n",
            "\n",
            "Input 4:\n",
            "Research indicates traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\n",
            "\n",
            "Extraction 4:\n",
            "Traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check installed versions of required packages\n",
        "!pip list | grep -E 'transformers|torch|accelerate|flash-attn'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMiPjqyUfUK8",
        "outputId": "e673c17f-42e0-462e-afd9-7c39cd4ec69f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                            0.31.0\n",
            "sentence-transformers                 3.4.1\n",
            "torch                                 2.3.1+cu121\n",
            "torchaudio                            2.3.1+cu121\n",
            "torchsummary                          1.5.1\n",
            "torchvision                           0.18.1+cu121\n",
            "transformers                          4.41.2\n"
          ]
        }
      ]
    }
  ]
}