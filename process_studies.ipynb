{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+x/swcxSINLh1F7woSctp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75e26b5b7f4b45ec9c079854156e1aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fad512bf31e742478c9fb2f06f977ba3",
              "IPY_MODEL_582027a3b5d941aaa52123b9047ced4c",
              "IPY_MODEL_35b4aed254ce468f8c3f3406be90cd17"
            ],
            "layout": "IPY_MODEL_0e5b7feaa81a46bfbdeb8ff37bc7d01a"
          }
        },
        "fad512bf31e742478c9fb2f06f977ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bda2e3e88bc4f669e9ae269a0a5c9ad",
            "placeholder": "​",
            "style": "IPY_MODEL_ca587730ff3041bb9b6972674affa366",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "582027a3b5d941aaa52123b9047ced4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a59cefba0441628c7a9d0ff977cb47",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0935a9f89d247929f7a75da82b799a2",
            "value": 2
          }
        },
        "35b4aed254ce468f8c3f3406be90cd17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0012cc2b29b4275936a17d707f93d87",
            "placeholder": "​",
            "style": "IPY_MODEL_2a137bbf6f9b493698eec5c6ec33682f",
            "value": " 2/2 [00:32&lt;00:00, 14.81s/it]"
          }
        },
        "0e5b7feaa81a46bfbdeb8ff37bc7d01a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bda2e3e88bc4f669e9ae269a0a5c9ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca587730ff3041bb9b6972674affa366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74a59cefba0441628c7a9d0ff977cb47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0935a9f89d247929f7a75da82b799a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0012cc2b29b4275936a17d707f93d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a137bbf6f9b493698eec5c6ec33682f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-jk/buyers-ID-manual-project/blob/main/process_studies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook extracts and cleans text from PDF reports for use in training the buyer profile extractor model.**"
      ],
      "metadata": {
        "id": "h3140H82TCGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add input and output examples to jsonl file**"
      ],
      "metadata": {
        "id": "aSaUOBcBZMvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alex-jk/buyers-ID-manual-project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUw6XCbS1p_W",
        "outputId": "f086d7b3-0567-40f9-a8dc-26e89e45784f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'buyers-ID-manual-project'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 76 (delta 32), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (76/76), 2.85 MiB | 8.46 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd buyers-ID-manual-project\n",
        "!ls"
      ],
      "metadata": {
        "id": "Gi3MKqB7cGcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbf6303-cfd8-44a6-a0a4-61f57f5407f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/buyers-ID-manual-project\n",
            "data\t\t\t      LICENSE\t\t     prompt_template.txt\n",
            "extraction_utils.py\t      process_studies.ipynb  README.md\n",
            "extract_text_from_pdfs.ipynb  prompt_main_idea.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "FHHdr7ZZTYe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specific compatible versions for torch 2.3.1 / cu121\n",
        "!pip install torch==2.3.1+cu121 torchaudio==2.3.1+cu121 torchvision==0.18.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install nltk\n",
        "\n",
        "# Install the other pinned versions\n",
        "!pip install transformers==4.41.2 accelerate==0.31.0 --no-deps\n",
        "\n",
        "# Install compatible tokenizers version LAST to ensure it sticks\n",
        "!pip install tokenizers==0.19.1\n",
        "\n",
        "# Ensure base dependencies are present (redundant ok)\n",
        "!pip install bitsandbytes sentencepiece\n",
        "\n",
        "print(\"\\nInstalled specific package versions (including compatible tokenizers). PLEASE RESTART RUNTIME NOW.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTTOXwDwcuaQ",
        "outputId": "2c80546d-8dc6-44fd-e888-7d2f5a93e063"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==2.3.1+cu121 in /usr/local/lib/python3.11/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchaudio==2.3.1+cu121 in /usr/local/lib/python3.11/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision==0.18.1+cu121 in /usr/local/lib/python3.11/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cu121) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cu121) (11.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1+cu121) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.1+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.1+cu121) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.11/dist-packages (4.41.2)\n",
            "Requirement already satisfied: accelerate==0.31.0 in /usr/local/lib/python3.11/dist-packages (0.31.0)\n",
            "Requirement already satisfied: tokenizers==0.19.1 in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.19.1) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.1.31)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "\n",
            "Installed specific package versions (including compatible tokenizers). PLEASE RESTART RUNTIME NOW.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import nltk"
      ],
      "metadata": {
        "id": "yCeel7fiPtID"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"NLTK 'punkt' tokenizer already downloaded.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
        "    nltk.download('punkt', quiet=True) # quiet=True suppresses verbose download output\n",
        "    print(\"NLTK 'punkt' downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej1ymoeNVwIO",
        "outputId": "cb846968-f93a-44f4-808c-ab58af43adeb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'punkt' tokenizer...\n",
            "NLTK 'punkt' downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read input output file and write to jsonl**"
      ],
      "metadata": {
        "id": "YPvesEjPT4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_df = pd.read_csv(\"data/buyers_id_manual_input_output.csv\")\n",
        "\n",
        "print(input_output_df.shape)\n",
        "print(input_output_df.columns)\n",
        "print(\"\\n-------------------------------\\n\")\n",
        "print(input_output_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u-YHG4rTcu7",
        "outputId": "5d150f42-b9d8-495b-b387-7ffd909f9452"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 3)\n",
            "Index(['Study', 'Input', 'Output'], dtype='object')\n",
            "\n",
            "-------------------------------\n",
            "\n",
            "                                    Study  \\\n",
            "0  The Shapiro Group Georgia Demand Study   \n",
            "1  The Shapiro Group Georgia Demand Study   \n",
            "2  The Shapiro Group Georgia Demand Study   \n",
            "3  The Shapiro Group Georgia Demand Study   \n",
            "4  The Shapiro Group Georgia Demand Study   \n",
            "\n",
            "                                               Input  \\\n",
            "0  Almost half these men are the age 30-39, with ...   \n",
            "1  The data clearly debunk the myth that CSEC is ...   \n",
            "2  Not only are 65% of men who buy sex with young...   \n",
            "3  Craigslist is by far the most efficient medium...   \n",
            "4  While many of the men who exploit these childr...   \n",
            "\n",
            "                                              Output  \n",
            "0  Almost half these men are the age 30-39, with ...  \n",
            "1  Men who\\nrespond to advertisements for sex wit...  \n",
            "2  65% of men who buy sex with young females do s...  \n",
            "3  Buyers respond to Craigslist ads 3 times more ...  \n",
            "4  Nearly half of buyers are willing to pay for s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_output_df = input_output_df.drop_duplicates(subset=[\"Study\", \"Input\", \"Output\"])\n",
        "\n",
        "# Convert to JSONL\n",
        "with open(\"data/labeled_chunks.jsonl\", \"w\") as f:\n",
        "    for _, row in input_output_df.iterrows():\n",
        "        json_obj = {\n",
        "            \"study\": row[\"Study\"],\n",
        "            \"input\": row[\"Input\"],\n",
        "            \"output\": row[\"Output\"]\n",
        "        }\n",
        "        f.write(json.dumps(json_obj) + \"\\n\")"
      ],
      "metadata": {
        "id": "jC56MIoNVPMZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models for text extraction**"
      ],
      "metadata": {
        "id": "z7s-W-gNZPi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/buyers-ID-manual-project\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bRDK6S1jYPu",
        "outputId": "9199059b-a233-4f0d-8b8a-cae9d47b623a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/buyers-ID-manual-project\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import extraction_utils\n",
        "\n",
        "# Reload the entire module\n",
        "importlib.reload(extraction_utils)\n",
        "print(\"Module 'extraction_utils' reloaded.\")\n",
        "\n",
        "from extraction_utils import extract_information, chunk_text_by_tokens, process_text_chunks_with_prompt\n",
        "\n",
        "print(\"Functions imported from extraction_utils successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRHpTCINVOxX",
        "outputId": "b09de1dd-a642-4329-b7a6-5d23af98afa8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module 'extraction_utils' reloaded.\n",
            "Functions imported from extraction_utils successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgJ0s70ynDtG",
        "outputId": "3c08154a-b219-4cee-cc43-bbbb6bd48048"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "YYtd4ZYUZRw2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check installed versions of required packages\n",
        "!pip list | grep -E 'transformers|torch|accelerate|flash-attn'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMiPjqyUfUK8",
        "outputId": "3942b319-a454-4d05-fbc2-1301725ddda9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                            0.31.0\n",
            "sentence-transformers                 3.4.1\n",
            "torch                                 2.3.1+cu121\n",
            "torchaudio                            2.3.1+cu121\n",
            "torchsummary                          1.5.1\n",
            "torchvision                           0.18.1+cu121\n",
            "transformers                          4.41.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# --- Load Model and Tokenizer ---\n",
        "# Load the model with 4-bit quantization to save memory (requires bitsandbytes)\n",
        "# Use device_map=\"auto\" to automatically use GPU if available\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",         # Use GPU if available, otherwise CPU\n",
        "        torch_dtype=\"auto\",        # Automatically select appropriate dtype\n",
        "        trust_remote_code=True,    # Phi-3 requires this\n",
        "        # Optional: uncomment below for 4-bit loading (needs bitsandbytes)\n",
        "        # load_in_4bit=True,\n",
        "        # bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    print(f\"Model '{model_id}' loaded successfully.\")\n",
        "\n",
        "    # --- Create a Hugging Face Pipeline for easier text generation ---\n",
        "    # Note: max_new_tokens controls how long the generated output can be. Adjust as needed.\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        # Adjust max_new_tokens if your extracted text might be longer\n",
        "        # Needs to be long enough for the longest expected extraction + \"NONE\"\n",
        "        max_new_tokens=256,\n",
        "        # Temperature=0 means more deterministic output, higher means more creative/random\n",
        "        # temperature=0.0,\n",
        "        # top_p=0.95, # Optional: nucleus sampling\n",
        "        do_sample=False # Set to False for more deterministic extraction\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or creating pipeline: {e}\")\n",
        "    print(\"Ensure you have sufficient RAM/VRAM and necessary libraries installed.\")\n",
        "    print(\"Consider using Google Colab with a T4 GPU runtime.\")\n",
        "    # Exit if model loading fails\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "75e26b5b7f4b45ec9c079854156e1aa5",
            "fad512bf31e742478c9fb2f06f977ba3",
            "582027a3b5d941aaa52123b9047ced4c",
            "35b4aed254ce468f8c3f3406be90cd17",
            "0e5b7feaa81a46bfbdeb8ff37bc7d01a",
            "4bda2e3e88bc4f669e9ae269a0a5c9ad",
            "ca587730ff3041bb9b6972674affa366",
            "74a59cefba0441628c7a9d0ff977cb47",
            "c0935a9f89d247929f7a75da82b799a2",
            "b0012cc2b29b4275936a17d707f93d87",
            "2a137bbf6f9b493698eec5c6ec33682f"
          ]
        },
        "id": "86Z4-8tTnLC_",
        "outputId": "17fc1e59-2acd-4b8f-d5ca-138a26218fb8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75e26b5b7f4b45ec9c079854156e1aa5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'microsoft/Phi-3-mini-4k-instruct' loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usage**"
      ],
      "metadata": {
        "id": "XywDoaLuZcBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_1 = \"Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\"\n",
        "\n",
        "input_text_2 = \"The data clearly debunk the myth that CSEC is a problem relegated to the urban core. Men who respond to advertisements for sex with young females come from all over metro Atlanta, the geographic market where the advertisements in this study were targeted.\"\n",
        "\n",
        "input_text_3 = \"This paragraph discusses unrelated economic factors in the region and contains no information about buyers or traffickers.\"\n",
        "\n",
        "input_text_4 = \"Research indicates traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\""
      ],
      "metadata": {
        "id": "uVOQqRnEZeOg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_buyers_profiles = \"prompt_template.txt\""
      ],
      "metadata": {
        "id": "6oc9NC9diERy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/buyers-ID-manual-project/extraction_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bljelaGjjEqQ",
        "outputId": "db9b126e-b48c-4005-ac62-ace0da34a61d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import os\n",
            "import re\n",
            "import nltk\n",
            "\n",
            "# --- Function to Load a Specific Prompt File ---\n",
            "def load_system_prompt(prompt_filename):\n",
            "    \"\"\"Loads system prompt text from a specified file.\"\"\"\n",
            "    system_prompt = \"\"\n",
            "    try:\n",
            "        # Assumes prompt file is in the same directory as this script\n",
            "        script_dir = os.path.dirname(__file__)\n",
            "        prompt_path = os.path.join(script_dir, prompt_filename)\n",
            "        with open(prompt_path, 'r', encoding='utf-8') as f:\n",
            "            system_prompt = f.read()\n",
            "        if not system_prompt:\n",
            "            print(f\"WARNING: Prompt file loaded but is empty: {prompt_path}\")\n",
            "            return f\"ERROR: Prompt file empty {prompt_filename}\" # Return specific error\n",
            "    except FileNotFoundError:\n",
            "        print(f\"ERROR: Prompt file not found at {prompt_path}\")\n",
            "        return f\"ERROR: Prompt file not found {prompt_filename}\" # Return specific error\n",
            "    except Exception as e:\n",
            "        print(f\"ERROR: Could not read prompt file {prompt_path}: {e}\")\n",
            "        return f\"ERROR: Could not read prompt file {prompt_filename}\" # Return specific error\n",
            "    return system_prompt\n",
            "\n",
            "# --- Function to Create Prompt Messages (Now uses loaded prompt) ---\n",
            "def create_prompt_messages(input_chunk, system_prompt_text):\n",
            "    \"\"\"Creates the chat message structure using the provided system prompt text.\"\"\"\n",
            "\n",
            "    # Check if system_prompt_text indicates an error from loading\n",
            "    if \"ERROR:\" in system_prompt_text:\n",
            "         print(f\"WARNING: Using error message as system prompt: {system_prompt_text}\")\n",
            "         # Optionally, you could return an error flag or raise an exception here too\n",
            "\n",
            "    # User prompt remains dynamic\n",
            "    user_prompt = f\"\"\"Now, perform the task for the following Input Text:\n",
            "\n",
            "Input Text:\n",
            "\"{input_chunk}\"\n",
            "\n",
            "Extracted Information:\n",
            "\"\"\"\n",
            "    messages = [\n",
            "        {\"role\": \"system\", \"content\": system_prompt_text}, # Use the passed-in system prompt\n",
            "        {\"role\": \"user\", \"content\": user_prompt},\n",
            "    ]\n",
            "    return messages\n",
            "\n",
            "# --- Function to Extract Information (Now accepts prompt filename) ---\n",
            "def extract_information(text_chunk, generation_pipeline, prompt_filename):\n",
            "    \"\"\"\n",
            "    Uses the pipeline to extract info based on the prompt specified by prompt_filename.\n",
            "    \"\"\"\n",
            "    if generation_pipeline is None or not callable(generation_pipeline):\n",
            "        print(\"Pipeline not initialized or not valid.\")\n",
            "        return \"Error: Pipeline not available or invalid.\"\n",
            "\n",
            "    # 1. Load the specific system prompt for this task\n",
            "    system_prompt_text = load_system_prompt(prompt_filename)\n",
            "    if \"ERROR:\" in system_prompt_text:\n",
            "         return f\"Error: Failed to load system prompt from '{prompt_filename}'.\" # Return early if prompt fails\n",
            "\n",
            "    # 2. Create messages using the loaded system prompt\n",
            "    messages = create_prompt_messages(text_chunk, system_prompt_text)\n",
            "\n",
            "    # 3. Call the pipeline and process output (same logic as before)\n",
            "    try:\n",
            "        outputs = generation_pipeline(messages, return_full_text=False)\n",
            "\n",
            "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
            "             generated_text = outputs[0]['generated_text'].strip()\n",
            "        else:\n",
            "             print(f\"Warning: Unexpected output format from pipeline: {outputs}\")\n",
            "             generated_text = \"Error: Unexpected pipeline output format.\"\n",
            "\n",
            "        if isinstance(generated_text, str): # Ensure it's a string before string methods\n",
            "            if generated_text.startswith('\"') and generated_text.endswith('\"'):\n",
            "                 generated_text = generated_text[1:-1]\n",
            "            if generated_text.startswith(\"'\") and generated_text.endswith(\"'\"):\n",
            "                 generated_text = generated_text[1:-1]\n",
            "            instruction_tail = \"Extracted Information:\"\n",
            "            if generated_text.startswith(instruction_tail):\n",
            "                generated_text = generated_text[len(instruction_tail):].strip()\n",
            "            if not generated_text or generated_text.lower() == \"none\":\n",
            "                return \"NONE\"\n",
            "\n",
            "        return generated_text\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"Error during generation pipeline call for {prompt_filename}: {e}\")\n",
            "        return f\"Error: Exception during generation. ({type(e).__name__})\"\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"Error during generation: {e}\")\n",
            "        return f\"Error: Could not generate output. ({e})\"\n",
            "\n",
            "# ----------------- create text chunks\n",
            "def chunk_text_by_tokens(text, tokenizer, max_chunk_tokens=3000, min_chunk_tokens=50):\n",
            "    \"\"\"\n",
            "    Splits text into chunks by combining complete sentences identified using NLTK,\n",
            "    ensuring each chunk's token count does not exceed max_chunk_tokens.\n",
            "    Handles sentences that individually exceed the token limit by truncating at word boundary.\n",
            "    \"\"\"\n",
            "    if not text or not hasattr(tokenizer, 'encode'):\n",
            "        print(\"ERROR: Text is empty or tokenizer is invalid/missing.\")\n",
            "        return []\n",
            "\n",
            "    try:\n",
            "        sentences = nltk.sent_tokenize(text)\n",
            "    except LookupError:\n",
            "         print(\"ERROR: NLTK 'punkt' tokenizer data not found. Ensure nltk.download('punkt') ran successfully.\")\n",
            "         return []\n",
            "    except Exception as e:\n",
            "         print(f\"ERROR: NLTK sentence tokenization failed: {e}\")\n",
            "         return []\n",
            "\n",
            "    chunks = []\n",
            "    current_chunk_sentences = []\n",
            "    current_chunk_tokens = 0\n",
            "\n",
            "    for i, sent in enumerate(sentences):\n",
            "        sent = sent.strip()\n",
            "        if not sent: continue\n",
            "\n",
            "        try:\n",
            "            sentence_tokens = len(tokenizer.encode(sent, add_special_tokens=False))\n",
            "        except Exception as e:\n",
            "            print(f\"    WARNING: Could not tokenize sentence {i+1}. Skipping. Error: {e}\")\n",
            "            continue\n",
            "\n",
            "        # --- Check if a SINGLE sentence is too long ---\n",
            "        if sentence_tokens > max_chunk_tokens:\n",
            "            print(f\"    WARNING: Sentence {i+1} (tokens: {sentence_tokens}) alone exceeds max_chunk_tokens ({max_chunk_tokens}). Truncating.\")\n",
            "            # Finalize previous chunk\n",
            "            if current_chunk_sentences:\n",
            "                chunk_text = \" \".join(current_chunk_sentences)\n",
            "                final_chunk_tokens = len(tokenizer.encode(chunk_text, add_special_tokens=False))\n",
            "                if final_chunk_tokens >= min_chunk_tokens: chunks.append(chunk_text)\n",
            "                else: print(f\"    INFO: Dropping previous short chunk (tokens: {final_chunk_tokens}) before oversized sentence.\")\n",
            "\n",
            "            # Truncate at word boundary\n",
            "            estimated_char_limit = max_chunk_tokens * 3 # Heuristic target\n",
            "            truncated_sent = sent\n",
            "            if len(sent) > estimated_char_limit:\n",
            "                last_space_index = sent.rfind(' ', 0, estimated_char_limit)\n",
            "                if last_space_index != -1: truncated_sent = sent[:last_space_index]\n",
            "                else: truncated_sent = sent[:estimated_char_limit] # Fallback\n",
            "\n",
            "            # Check truncated token count\n",
            "            truncated_tokens = len(tokenizer.encode(truncated_sent, add_special_tokens=False))\n",
            "            if truncated_tokens > max_chunk_tokens:\n",
            "                print(f\"    ERROR: Sentence still exceeds token limit ({truncated_tokens}) after truncation. Skipping.\")\n",
            "            elif truncated_tokens >= min_chunk_tokens:\n",
            "                chunks.append(truncated_sent)\n",
            "                print(f\"    INFO: Added truncated sentence chunk (tokens: {truncated_tokens}).\")\n",
            "            else:\n",
            "                print(f\"    INFO: Truncated sentence too short (tokens: {truncated_tokens}). Skipping.\")\n",
            "\n",
            "            # Reset and continue\n",
            "            current_chunk_sentences = []\n",
            "            current_chunk_tokens = 0\n",
            "            continue\n",
            "        # ---------------------------------------------\n",
            "\n",
            "        potential_chunk_tokens = current_chunk_tokens + sentence_tokens + (1 if current_chunk_sentences else 0)\n",
            "\n",
            "        if current_chunk_sentences and potential_chunk_tokens > max_chunk_tokens:\n",
            "            # Finalize current chunk\n",
            "            chunk_text = \" \".join(current_chunk_sentences)\n",
            "            final_chunk_tokens = len(tokenizer.encode(chunk_text, add_special_tokens=False))\n",
            "            if final_chunk_tokens >= min_chunk_tokens: chunks.append(chunk_text)\n",
            "            else: print(f\"    INFO: Dropping short chunk (tokens: {final_chunk_tokens}).\")\n",
            "\n",
            "            # Start new chunk\n",
            "            current_chunk_sentences = [sent]\n",
            "            current_chunk_tokens = sentence_tokens\n",
            "        else:\n",
            "            # Add sentence to current chunk\n",
            "            current_chunk_sentences.append(sent)\n",
            "            current_chunk_tokens = potential_chunk_tokens # Approximate\n",
            "\n",
            "    # Add the last chunk\n",
            "    if current_chunk_sentences:\n",
            "        chunk_text = \" \".join(current_chunk_sentences)\n",
            "        final_chunk_tokens = len(tokenizer.encode(chunk_text, add_special_tokens=False))\n",
            "        if final_chunk_tokens >= min_chunk_tokens: chunks.append(chunk_text)\n",
            "        else: print(f\"    INFO: Dropping final short chunk (tokens: {final_chunk_tokens}).\")\n",
            "\n",
            "    print(f\"Split text into {len(chunks)} chunks using NLTK sentences (max tokens: {max_chunk_tokens}).\")\n",
            "    return chunks\n",
            "\n",
            "# --- Function to Process Chunks ---\n",
            "def process_text_chunks_with_prompt(text_chunks, generation_pipeline, prompt_filename_to_use):\n",
            "    \"\"\"Processes each text chunk using the extraction pipeline and a specific prompt.\"\"\"\n",
            "    all_extractions = []\n",
            "    if not callable(generation_pipeline):\n",
            "        print(\"ERROR: Generation pipeline is not available or not valid.\")\n",
            "        return []\n",
            "    if not text_chunks:\n",
            "        print(\"ERROR: No text chunks provided.\")\n",
            "        return []\n",
            "\n",
            "    num_chunks = len(text_chunks)\n",
            "    print(f\"\\nProcessing {num_chunks} chunks using prompt '{prompt_filename_to_use}'...\")\n",
            "\n",
            "    for i, chunk in enumerate(text_chunks):\n",
            "        # Print chunk length (optional debug)\n",
            "        print(f\"  Processing chunk {i+1}/{num_chunks}. Length: {len(chunk)} characters (Token count check happens internally).\")\n",
            "\n",
            "        # Call the main extraction function\n",
            "        extraction_result = extract_information(chunk, generation_pipeline, prompt_filename_to_use)\n",
            "\n",
            "        if extraction_result and extraction_result.strip().upper() != \"NONE\" and \"Error:\" not in extraction_result :\n",
            "            print(f\"    Found relevant info in chunk {i+1}: {extraction_result[:100]}...\")\n",
            "            all_extractions.append(extraction_result)\n",
            "        elif \"Error:\" in extraction_result:\n",
            "             print(f\"    ERROR processing chunk {i+1}: {extraction_result}\")\n",
            "             # Optionally store errors: all_extractions.append(f\"CHUNK {i+1} ERROR: {extraction_result}\")\n",
            "        else:\n",
            "             print(f\"    No relevant info found in chunk {i+1}.\")\n",
            "\n",
            "    print(f\"\\nFinished processing chunks. Found {len(all_extractions)} successful relevant extractions for '{prompt_filename_to_use}'.\")\n",
            "    return all_extractions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Extracting Information ---\")\n",
        "\n",
        "# Check if the pipeline object exists before trying to use it\n",
        "if 'pipe' in locals() and pipe is not None:\n",
        "    print(f\"\\nRunning prompt: {prompt_buyers_profiles}\")\n",
        "\n",
        "    print(f\"\\nInput 1:\\n{input_text_1}\")\n",
        "    extraction_1 = extract_information(input_text_1, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 1:\\n{extraction_1}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 2:\\n{input_text_2}\")\n",
        "    extraction_2 = extract_information(input_text_2, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 2:\\n{extraction_2}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 3:\\n{input_text_3}\")\n",
        "    extraction_3 = extract_information(input_text_3, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 3:\\n{extraction_3}\")\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    print(f\"\\nInput 4:\\n{input_text_4}\")\n",
        "    extraction_4 = extract_information(input_text_4, pipe, prompt_buyers_profiles)\n",
        "    print(f\"\\nExtraction 4:\\n{extraction_4}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: The 'pipe' object (text generation pipeline) was not found or not created successfully.\")\n",
        "    print(\"Please ensure the model loading cell was run successfully after importing functions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju44xbqscRuN",
        "outputId": "7d5ece2b-ad4d-4fa7-aa4c-5aa43949cbe7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extracting Information ---\n",
            "\n",
            "Running prompt: prompt_template.txt\n",
            "\n",
            "Input 1:\n",
            "Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extraction 1:\n",
            "Almost half these men are the age 30-39, with the next largest group being men under age 30. The mean age is 33 and the median 31. The youngest survey participant was 18, and the oldest was 67.\n",
            "--------------------\n",
            "\n",
            "Input 2:\n",
            "The data clearly debunk the myth that CSEC is a problem relegated to the urban core. Men who respond to advertisements for sex with young females come from all over metro Atlanta, the geographic market where the advertisements in this study were targeted.\n",
            "\n",
            "Extraction 2:\n",
            "Men who respond to advertisements for sex with young females come from all over metro Atlanta.\n",
            "--------------------\n",
            "\n",
            "Input 3:\n",
            "This paragraph discusses unrelated economic factors in the region and contains no information about buyers or traffickers.\n",
            "\n",
            "Extraction 3:\n",
            "NONE\n",
            "--------------------\n",
            "\n",
            "Input 4:\n",
            "Research indicates traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\n",
            "\n",
            "Extraction 4:\n",
            "Traffickers often groom potential buyers by displaying luxury goods online and frequenting specific forums known for risky behavior discussions. Victims may appear withdrawn or display signs of coaching. Reporting suspicions anonymously through the national hotline is encouraged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Articles - extract main ideas**"
      ],
      "metadata": {
        "id": "n35nMBV_yW1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DEFINE INPUT and PROMPT FILENAMES\n",
        "input_txt_filename = \"buyers-manual-article-01.txt\"  # <--- CHANGE to your uploaded .txt filename\n",
        "prompt_to_use = \"prompt_main_idea.txt\"    # <--- CHANGE to the prompt file you want to apply\n",
        "\n",
        "input_txt_filepath = os.path.join('/content/buyers-ID-manual-project', input_txt_filename)"
      ],
      "metadata": {
        "id": "MGr5SS-HybjZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq6T_OMPE0us",
        "outputId": "ff8f02a1-9e6f-486e-893a-357e5f77f00f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buyers-ID-manual-project  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check if pipeline 'pipe' exists from previous cells\n",
        "prompts_to_run = [\n",
        "    \"prompt_main_idea.txt\"\n",
        "]\n",
        "\n",
        "CHUNK_MAX_TOKENS = 2500\n",
        "\n",
        "if 'pipe' in locals() and callable(pipe) and 'tokenizer' in locals() and hasattr(tokenizer, 'encode'):\n",
        "    print(f\"Pipeline and tokenizer are ready.\")\n",
        "    # --- Read Input File ---\n",
        "    print(f\"Reading text file: {input_txt_filepath}\")\n",
        "    try:\n",
        "        with open(input_txt_filepath, 'r', encoding='utf-8') as f:\n",
        "            full_text_content = f.read()\n",
        "        print(f\"Successfully read {len(full_text_content)} characters.\")\n",
        "\n",
        "        if full_text_content:\n",
        "            # --- Chunk Text ---\n",
        "            # Calls the function imported from extraction_utils.py\n",
        "            print(f\"Chunking text with max_chunk_tokens = {CHUNK_MAX_TOKENS}...\")\n",
        "            text_chunks = chunk_text_by_tokens(full_text_content, tokenizer, max_chunk_tokens=CHUNK_MAX_TOKENS)\n",
        "\n",
        "            if text_chunks:\n",
        "                #  --- Optional Debug: Print Chunk Token Counts ---\n",
        "                 print(\"\\n--- Checking Chunk Token Counts ---\")\n",
        "                 for i, chunk in enumerate(text_chunks):\n",
        "                     try:\n",
        "                         chunk_tokens = len(tokenizer.encode(chunk, add_special_tokens=False))\n",
        "                         print(f\"Chunk {i+1} token count: {chunk_tokens}\")\n",
        "                     except Exception as e: print(f\"Could not tokenize chunk {i+1}: {e}\")\n",
        "                 print(\"------------------------------------\\n\")\n",
        "                #  -------------------------------------------\n",
        "\n",
        "                 # --- Process Chunks for Each Prompt ---\n",
        "                 all_results_by_prompt = {} # Dictionary to store results per prompt\n",
        "                 for prompt_file in prompts_to_run:\n",
        "                     # Calls the function imported from extraction_utils.py\n",
        "                     final_results = process_text_chunks_with_prompt(text_chunks, pipe, prompt_file)\n",
        "                     all_results_by_prompt[prompt_file] = final_results\n",
        "\n",
        "                 # --- Display All Aggregated Results ---\n",
        "                 print(\"\\n\\n\" + \"=\"*40)\n",
        "                 print(\"          FINAL AGGREGATED RESULTS\")\n",
        "                 print(\"=\"*40)\n",
        "                 for prompt_file, results in all_results_by_prompt.items():\n",
        "                      print(f\"\\n--- Results for Prompt: {prompt_file} ---\")\n",
        "                      if results:\n",
        "                          for i, result in enumerate(results):\n",
        "                              print(f\"Extraction {i+1}:\\n{result}\\n{'-'*20}\")\n",
        "                      else:\n",
        "                          print(f\"No relevant information extracted using '{prompt_file}'.\")\n",
        "                      print(\"=\"*30)\n",
        "\n",
        "            else:\n",
        "                print(\"Text file was read, but no chunks were created (check chunking logic/content).\")\n",
        "        else:\n",
        "            print(\"Text file is empty.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Input text file not found at '{input_txt_filepath}'. Check filename and current directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print detailed traceback for debugging\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: The 'pipe' object and/or 'tokenizer' object not found or not ready.\")\n",
        "    print(\"Please ensure the Model Loading cell (Cell 5) was run successfully BEFORE this cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAOF1G0y5BkO",
        "outputId": "5df99915-6e81-4db3-b472-91048db85bdf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline and tokenizer are ready.\n",
            "Reading text file: /content/buyers-ID-manual-project/buyers-manual-article-01.txt\n",
            "Successfully read 65405 characters.\n",
            "Chunking text with max_chunk_tokens = 2500...\n",
            "ERROR: NLTK 'punkt' tokenizer data not found. Ensure nltk.download('punkt') ran successfully.\n",
            "Text file was read, but no chunks were created (check chunking logic/content).\n"
          ]
        }
      ]
    }
  ]
}